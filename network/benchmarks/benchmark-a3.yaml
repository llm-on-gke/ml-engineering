# Distributed training of a traditional CNN model to do image classification 
# using the MNIST dataset and PyTorch.
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: pytorch
spec:
  replicatedJobs:
  - name: workers
    template:
      spec:
        parallelism: 2
        completions: 2
        backoffLimit: 0
        template:
          spec:
            nodeSelector:
             cloud.google.com/gke-accelerator: nvidia-h100-80gb
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet
            containers:
            - name: tcpx-daemon 
              image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/tcpgpudmarxd-dev:v2.0.9
              imagePullPolicy: Always
              command:
                - /tcpgpudmarxd/build/app/tcpgpudmarxd
                - --gpu_nic_preset
                - a3vm
                - --gpu_shmem_type
                - fd
                - --uds_path
                - /run/tcpx
                - --setup_param
                - \"--verbose 128 2 0 \"
              securityContext:
                privileged: true
              volumeMounts:
               - name: libraries
                 mountPath: /usr/local/nvidia/lib64
                 readOnly: true
               - name: tcpx-socket
                 mountPath: /run/tcpx
              env:
               - name: LD_LIBRARY_PATH
                 value: /usr/local/nvidia/lib64
            - name: pytorch
              image: us-east1-docker.pkg.dev/rick-vertex-ai/gke-llm/torch-benchmark:latest
              ports:
              - containerPort: 3389
              securityContext:
                privileged: true
              env:
              - name: MASTER_ADDR
                value: "pytorch-workers-0-0.pytorch"
              - name: MASTER_PORT
                value: "3389"
              - name: LOCAL_RANK
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              # Force python to not buffer output and write directly to stdout, so we can view training logs via `kubectl logs`.
              - name: PYTHONUNBUFFERED
                value: "0"
              - name: LD_LIBRARY_PATH
                value: /usr/local/nvidia/lib64
              resources:
                limits:
                  nvidia.com/gpu: 8
              command:
              - bash
              - -xc
              - |
                python -u -m torch.distributed.run \
                --nproc_per_node 8 \
                --nnodes 2 \
                --rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
                --rdzv_backend c10d \
                --max_restarts 0 \
                --role $MASTER_ADDR: \
                --tee 3 \
                all_reduce_bench.py
                #torchrun --rdzv_id=123 --nnodes=2 --nproc_per_node=8 --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT --node_rank=$RANK all_reduce_bench.py --epochs=1 --log-interval=1  
              volumeMounts:
               - name: tcpx-socket
                 mountPath: /tmp
               - name: config-volume
                 mountPath: /configs
            volumes:
            - name: config-volume
              configMap:
               name: nccl-configmap
               defaultMode: 0777
            - name: libraries
              hostPath:
               path: /home/kubernetes/bin/nvidia/lib64
            - name: tcpx-socket
              emptyDir: {}